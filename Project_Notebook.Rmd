---
title: "Time Series Analysis"
author: "Ramnath K (22BM6JP38), Vaibhav Goyal (22BM6JP54)"
date: "`r Sys.Date()`"
abstract: "This project is a part of our course Stochastic Processes and Applications at ISI, Kolkata. We will analyse the monthly sales of new cars sales in the USA from January 1992 to October 2022. We have obtained this data from the Monthly Retail Trade Report published by the United States Census Bureau. The objective of this project is to understand all the basic concepts of time series analysis, and by the end, be able to fit a suitable time series model and forecast the sales for a certain period. The repository of this project can be found at https://github.com/vai-b-hav/SPA_Project"
knit: (function(inputFile, encoding) {
      out_dir <- "Report";
      rmarkdown::render(inputFile,
                        encoding=encoding,
                        output_dir=file.path(dirname(inputFile), out_dir))})
output: word_document
---

---
# Data is fetched from the Data subfolder.
# Knitted Output of the notebook will be stored in the Report subfolder.
---

***

# Initial Setup

#### Import Libraries:

We need to import the following libraries for the project:

1.  *astsa* - for Time Series Analysis
2.  *readxl* - for importing data from Excel file

```{r lib import}
library(astsa)
library(readxl)
```

```{r output setup, include=FALSE}
# Flextable imported to create tables in output file
library(flextable)
# Table caption set to bottom
knitr::opts_chunk$set(tab.topcaption = FALSE)
# Avoid scientific notations in the plots
options(scipen = 999)
```


#### Import Data
```{r data import}
data <- read_xlsx("./Data/US_MRTR.xlsx", sheet = 'Sales')
```
```{r table code, echo=FALSE}
ft <- 
  flextable(head(data)) |>
  set_table_properties(layout = "autofit") |>
  set_caption(caption = "Sample from data") |>
  colformat_num(j = 'Year', big.mark = "")
ft
```

Before proceeding, let us perform some minor transformations:

1.  Change name of the sales column for easier reference.
2.  Change unit of the sales from Millions to Billions.

```{r data tansform}
colnames(data)[3] <- 'Sales'
data$Sales <- data$Sales / 1000 # Change scale from Millions to Billions
```

```{r table_2 code, echo=FALSE}
ft <- 
  flextable(head(data)) |>
  set_table_properties(layout = "autofit") |>
  set_caption(caption = "Sample from data after transformations") |>
  colformat_num(j = 'Year', big.mark = "")
ft
```

***

# Analysis

#### Define time series

The *ts()* function from the *astsa* helps us define an indexed time series from out sales data.

Sometimes when plotting the data, it is helpful to have the actual time intervals on the on the axis, for this purpose we also create a time series indexed by the actual time intervals. *(sales.ts in the below code)*
```{r ts}
sales <- ts(data = data$Sales)
sales.ts <- ts(data = data$Sales, frequency = 12, start = c(1992,1))
```

#### Plot time series

```{r plot, echo=FALSE, fig.width=13, fig.height=6}
plot(sales.ts, type="o", pch = 20,
     ylab = "Sales (in $ 'B)",
     main = "Monthly New Car Sales in US [Jan 1992 - Oct 2022]")
rect(xleft=2008,xright = 2010, ybottom = par("usr")[3], ytop=par("usr")[4], density=10, col = "red")
rect(xleft=2020,xright = 2021, ybottom = par("usr")[3], ytop=par("usr")[4], density=10, col = "blue")
```

On plotting the time series data, we observe a clear increasing trend and a seasonality in the data.

We observe a departure from the general trend for the years 2008 to 2010 which may be attributed to the global economic crisis of 2008. *(Highlighted in red region in the above plot)*

We also observe a sharp drop in the sales in the years 2020 to 2021 which can be attributed to the CoVID-19 pandemic. *(Highlighted in blue region in the above plot)*

For our initial initial analysis, we will ignore these two anomalies and proceed with the data as is.

## AR(1) Model

Since there is upward trend, we first need to make it stationary to fit an AR(1) model. To create a (possibly) stationary series, weâ€™ll examine the first differences $y_t = x_t - x_{t-1}$.

```{r}
sales.diff <- diff(sales, 1)
```

The time series plot of the first differences is as follows:

```{r, echo=FALSE, fig.width=9, fig.height=6}
plot(sales.diff, type="o", pch = 20,
     ylab = expression("Sales"["t"]~-~"Sales"["t-1"]),
     xlab = "Index",
     main = "Time Series Plot of Sales Differences")
```

And, the following plot is the sample estimate of the autocorrelation function of 1st differences:

```{r, echo=FALSE, fig.width=10, fig.height=3}
acf(sales.diff,xlim=c(1,24))
```

In the ACF plot we observe that there are multiple lag values that have significant ACFs. Thus an AR(1) model will not be a suitable choice for this data.

If we still consider having a look at the plots of the AR(1) model, they would be as follows:

```{r, echo=FALSE, fig.width=9, fig.height=5}
sales.diff.lag1 <- lag(sales.diff,-1)
y <- cbind(sales.diff,sales.diff.lag1)
ar1.sales.diff <- lm(y[,1] ~ y[,2])
par(mfrow=c(1,3))
plot(y[,2], y[,1],
     main = 'AR(1) of Sales Differences',
     xlab = "Sales Differnce (t-1)",
     ylab = "Sales Differnce (t)")
abline(ar1.sales.diff, col = 'red')
plot(ar1.sales.diff$fit,ar1.sales.diff$residuals,
     main = 'Residual Plot',
     xlab = 'Fitted Value',
     ylab = 'Residual')
acf(ar1.sales.diff$residuals, xlim = c(1,24), main = '')
title(main = 'ACF of Residuals', line = 0.5)
```

From the first scatter plot and the residual plot, we can clearly see that AR(1) model is not a good fit. Also, from the ACF plot, it is evident that the residuals still have significant auto-correlation.  

## SARIMA Model

Since we have a monthly data that exhibits seasonality, we should first observe the plot for
$${Z_{t} = \nabla_{12}x_{t} = x_t - x_{t-12}}$$
The idea is that differences from the previous year may be, on average, about the same for each month of a year.

```{r}
sales.diff.12 <- diff(sales, 12)
```


```{r, echo=FALSE, fig.width=9, fig.height=6}
plot(sales.diff.12, type = 'o', pch = 20,
     ylab = expression("Sales"["t"]~-~"Sales"["t-12"]),
     xlab = 'Index',
     main = "Time Series Plot of Sales Differences")
```

Here we see that the data has been de-trended, thus, further differencing is not necessary.

Since we have kept the data of 2008 economic crisis as well as the CoVID-19 pandemic, we see there are a lot of outliers in the plot. We will still try to fit a SARIMA model. For the identification of the SARIMA model, we need to have a look at the ACF and PACF plot of the de-trended series. 

```{r ,results='hide',fig.width=9, fig.height=6}
acf2(sales.diff.12,48)
```

Based on the above plots let us try to identify the suitable model:

**Non-seasonal component**

We notice that ACF for early lags are tapering and there are spikes in the early lags of PACF.This indicates that the non seasonal component could be an AR(1) or an AR(2).

**Seasonal component**

We notice that the PACFs for lags that are multiple of 12 are tapering and there are no significant for their ACFs. Thus the seasonal component at best could be either MA(1) or MA(2) along with a first order differencing.

Based on these observations, we can try the following models-

|        ${ARIMA(1,0,0)\times(0,1,1)_{12}}$
|        ${ARIMA(2,0,0)\times(0,1,1)_{12}}$
|        ${ARIMA(1,0,0)\times(0,1,2)_{12}}$
|        ${ARIMA(2,0,0)\times(0,1,1)_{12}}$

Of these, we find that the best fit model is $ARIMA(2,0,0)~\times~(0,1,1)$ with an AIC of 1956.02.

Following plots can further help us asses the appropriateness of this model: 
```{r, results='hide', fig.width=9, fig.height=6}
sarima(sales, 2,0,0, 0,1,1,12)
```

We can clearly see that that all the p values for the Ljung-Box statistic for all the lags are significant. This shows that the model is not a good fit.

One of the main cause for the problems we are facing while fitting the model could be that we have until now taken to whole series from Jan 1992 to Oct 2022 which also includes data from the period of 2008 economic recession and the CoVID-19 pandemic.

## SARIMA Model on a subset of data

For the purpose of understanding the concepts we can take subset of data from January 2010 to December 2019. From this we can then fit a model on Jan 2010 to Dec 2017, forecast the sales for Jan 2018 to Dec 2019 and use the actaul data from that period to very our results.


```{r}
sales10to17 <- ts(data$Sales[217:312])
sales18to19 <- ts(data$Sales[313:337])

# Also creating ts with actual time intervals for plots later. 
sales10to17.ts <- ts(data$Sales[217:312], frequency = 12, start = c(2010,1))
sales18to19.ts <- ts(data$Sales[313:337], frequency = 12, start = c(2018,1))
```

Let us now have a look at the data we have.

```{r, echo=FALSE, fig.width=9, fig.height=6}
plot(sales10to17.ts, type = "o", pch = 20,
     ylab = "Sales (in $ 'B)",
     main = "Monthly New Car Sales in US [Jan 2010 - Dec 2017]")
```

Now, we should again start our analysis for identifying the SARIMA model by first performing the differencing- $\nabla_{12}x_t$.

```{r}
sales10to17.diff.12 <- diff(sales10to17, 12)
```
```{r, echo=FALSE, fig.width=9, fig.height=6}
plot(sales10to17.diff.12, type = "o", pch = 20,
     ylab = expression("Sales"["t"]~-~"Sales"["t-12"]),
     xlab = 'Index',
     main = "Time Series Plot of Sales Differences")
```

We see that this still have a downward trend in the data, we should thus consider performing another differencing - $\nabla_{1}x_t$.

```{r}
sales10to17.diff.12.1 <- diff(sales10to17.diff.12, 1)
```

```{r, echo=FALSE, fig.width=9, fig.height=6}
plot(sales10to17.diff.12.1, type = "o", pch = 20,
     ylab = expression("Z"["t"]~-~"Z"["t-1"]),
     xlab = 'Index',
     main = "Time Series Plot of Sales Differences")
```

The series now appears to be somewhat stationary. We should now have a look at the ACFs and the PACFs.
```{r, results='hide'}
acf2(sales10to17.diff.12.1, 48)
```

Based on the above plots let us try to identify the suitable model:

**Non-seasonal component**

We notice that PACF for early lags are tapering and there are spikes in the early lags of ACF.This indicates that the non seasonal component could be an MA(1) or an MA(2) along with a first order differencing.

**Seasonal component**

We notice that the neither tha ACFs nor the PACFs for lags that are multiple of 12 are significant for their ACFs. Thus the seasonal component has nothing apart from a first order differencing.

Thus the models that we can try are-

|        ${ARIMA(0,1,1)\times(0,1,0)_{12}}$
|        ${ARIMA(0,1,2)\times(0,1,0)_{12}}$

Of these, we find that the first model has a lower AIC of 354.34. Also the p value for the second MA term in the second model is more than 0.05 it is therefore not significant. Thus we finally pick the model- ${ARIMA(0,1,1)\times(0,1,0)_{12}}$.

Following plots can further help us asses the appropriateness of this model: 
```{r, results='hide', fig.width=9, fig.height=6}
sarima(sales10to17, 0,1,1, 0,1,0, 12)
```

We can see that the value of standardized residual is low; the ACFs of residuals are also not significant; the Q-Q plot indicates good fit; and, the p values for the Ljung-Box statistics are greater than 0.05 for all the lags. Thus we can conclude that the model is a good fit.

Let us now forecast data for 2018 and 2019 using this model.
```{r, results='hide',fig.width=9, fig.height=3.75}
sarima.for(sales10to17.ts, 25, 0,1,1, 0,1,0, 12, pch = 16, lwd = 1)
```

We can also overlay the actual values to assess the accuracy of our forecasts.

```{r, results='hide',echo = FALSE,fig.width=9, fig.height=3.75}
sarima.for(sales10to17.ts, 25, 0,1,1, 0,1,0, 12, lwd = 1, pch=16)
lines(sales18to19.ts, lwd = 1, lty = 2, col = 'midnightblue', type="o", pch = 4)
legend('topleft'
       , legend=c("Time Series", "Forecast", "Actual Value")
       , pch = c(16, 16, 4)
       , lty = c(1, 1, 2)
       , lwd = c(1, 1, 1)
       , col=c("black", "red", 'midnightblue')
       , cex = 1)
```

From the plot we can see that the actual values for most of the month are within 1 or 2 standard error of the forecast.